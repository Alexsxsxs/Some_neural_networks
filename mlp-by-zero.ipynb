{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Optional, List","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.081141Z","iopub.execute_input":"2025-09-30T12:41:32.081446Z","iopub.status.idle":"2025-09-30T12:41:32.090731Z","shell.execute_reply.started":"2025-09-30T12:41:32.081419Z","shell.execute_reply":"2025-09-30T12:41:32.089579Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class Layer:\n\n    def __init__(self):\n        self.training = True\n\n    def forward(self, x):\n        pass\n\n    def backward(self, grad_output):\n        pass\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n\n    def __call__(self, x):\n        return self.forward(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.092323Z","iopub.execute_input":"2025-09-30T12:41:32.092586Z","iopub.status.idle":"2025-09-30T12:41:32.124747Z","shell.execute_reply.started":"2025-09-30T12:41:32.092564Z","shell.execute_reply":"2025-09-30T12:41:32.123763Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class RBF(Layer):\n\n    def __init__(self, input_size, output_size, bias=True):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = bias\n\n        self.weight = np.random.normal(loc=0, scale=2/input_size, size=(input_size, output_size)).astype(np.float32)\n\n        if self.use_bias:\n            self.bias = np.zeros(output_size, dtype=np.float32)\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        self.input = x\n\n        output = np.square(self.input - self.weight)\n        if self.use_bias:\n            output = output - np.square(self.bias)\n        return output\n\n    def backward(self, grad_output):\n        grad_input = grad_output.T @ (2 * self.input - 2 * self.weight).T\n\n        self.grad_weight = grad_output @ (-2 * self.input + 2 * self.weight).T\n\n        if self.use_bias:\n            self.grad_bias = np.sum(grad_output, axis=0) * (2 * self.bias).T\n\n        return grad_input\n\n    def update_weights(self, learning_rate=0.01):\n        if self.grad_weight is not None:\n            self.weight.T -= learning_rate * self.grad_weight\n\n        if self.use_bias and self.grad_bias is not None:\n            self.bias -= learning_rate * self.grad_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.126167Z","iopub.execute_input":"2025-09-30T12:41:32.126522Z","iopub.status.idle":"2025-09-30T12:41:32.153382Z","shell.execute_reply.started":"2025-09-30T12:41:32.126476Z","shell.execute_reply":"2025-09-30T12:41:32.152030Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"### –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU\n\nclass ReLU(Layer):\n\n    def __init__(self):\n        super().__init__()\n        self.input = None\n\n    def forward(self, x):\n        self.input = x\n        output = np.maximum(0, x)\n        return output\n\n    def backward(self, grad_output):\n        grad_input = grad_output.copy()\n        grad_input[self.input <= 0] = 0 ### –ü—Ä–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ReLU –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä—è–º—É—é, –µ—ë –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –Ω–æ–ª—å\n                                   ### –ü—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏—è—Ö –±–æ–ª—å—à–µ –Ω—É–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä—è–º—É—é —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º —É–≥–ª–∞ –Ω–∞–∫–ª–æ–Ω–∞ 1\n                                   ### –ï—Å–ª–∏ —è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–º–Ω—é –≤ —á–µ–º —Å–º—ã—Å–ª backward —É —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Ç–æ –º—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –Ω–∞ grad\n                                   ### –¢–æ –µ—Å—Ç—å –∫–æ–≥–¥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ä–∞–≤–Ω–∞ 1, –º—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ –≤–µ—Å–∞\n        return grad_input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.155246Z","iopub.execute_input":"2025-09-30T12:41:32.155590Z","iopub.status.idle":"2025-09-30T12:41:32.177349Z","shell.execute_reply.started":"2025-09-30T12:41:32.155549Z","shell.execute_reply":"2025-09-30T12:41:32.176158Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# –¢–µ—Å—Ç ReLU (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ ReLU)\nrelu = ReLU()\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\nx_test = np.array([[-2, -1, 0, 1, 2]], dtype=np.float32)\nexpected_forward = np.array([[0, 0, 0, 1, 2]], dtype=np.float32)\n\n# # Forward pass\noutput = relu.forward(x_test)\nprint(f\"Input: {x_test}\")\nprint(f\"Output: {output}\")\nprint(f\"Expected: {expected_forward}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∫–∞ forward pass\nassert np.allclose(output, expected_forward), \"ReLU forward pass –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\"\n\n# # Backward pass\ngrad_output = np.ones_like(output)\ngrad_input = relu.backward(grad_output)\nexpected_backward = np.array([[0, 0, 0, 1, 1]], dtype=np.float32)\n\nprint(f\"Gradient output: {grad_output}\")\nprint(f\"Gradient input: {grad_input}\")\nprint(f\"Expected gradient: {expected_backward}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∫–∞ backward pass\nassert np.allclose(grad_input, expected_backward), \"ReLU backward pass –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\"\n\nprint(\"‚úÖ ReLU —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ ReLU –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.179651Z","iopub.execute_input":"2025-09-30T12:41:32.179937Z","iopub.status.idle":"2025-09-30T12:41:32.218211Z","shell.execute_reply.started":"2025-09-30T12:41:32.179914Z","shell.execute_reply":"2025-09-30T12:41:32.216961Z"}},"outputs":[{"name":"stdout","text":"Input: [[-2. -1.  0.  1.  2.]]\nOutput: [[0. 0. 0. 1. 2.]]\nExpected: [[0. 0. 0. 1. 2.]]\nGradient output: [[1. 1. 1. 1. 1.]]\nGradient input: [[0. 0. 0. 1. 1.]]\nExpected gradient: [[0. 0. 0. 1. 1.]]\n‚úÖ ReLU —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"### –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Sigmoid\n\nclass Sigmoid(Layer):\n    def __init__(self):\n        super().__init__()\n        self.output = None\n\n    def forward(self, x):\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, grad_output):\n        ### –ò–∑ —á–∏—Å—Ç–æ–≥–æ –∏–Ω—Ç–µ—Ä–µ—Å–∞ –±—Ä–∞–ª –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é, –≤—ã—Ö–æ–¥–∏—Ç (–µ^(-x)/(1+–µ^(-x))^2)\n        ### –¢–∞–∫ –ø–æ–Ω–∏–º–∞—é –∏–º–µ–Ω–Ω–æ –µ—ë –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ —Å–∏–ª—É —Ç–æ–≥–æ, —á—Ç–æ –ø–æ—è–≤–ª—è–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å\n        return grad_output * self.output * (1 - self.output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.219341Z","iopub.execute_input":"2025-09-30T12:41:32.220153Z","iopub.status.idle":"2025-09-30T12:41:32.250385Z","shell.execute_reply.started":"2025-09-30T12:41:32.220093Z","shell.execute_reply":"2025-09-30T12:41:32.249317Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# –¢–µ—Å—Ç Sigmoid (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Sigmoid)\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Sigmoid –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\nsigmoid = Sigmoid()\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\nx_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n\n# # Forward pass\noutput = sigmoid.forward(x_test)\nprint(f\"Input: {x_test}\")\nprint(f\"Output: {output}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (0, 1)\nassert np.all(output > 0) and np.all(output < 1), \"Sigmoid –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (0, 1)\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç—å: sigmoid(-x) = 1 - sigmoid(x)\nx_sym = np.array([[1]], dtype=np.float32)\nout_pos = sigmoid.forward(x_sym)\nout_neg = sigmoid.forward(-x_sym)\nassert np.allclose(out_neg, 1 - out_pos, atol=1e-6), \"Sigmoid –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º\"\n\n# # Backward pass\ngrad_output = np.ones_like(output)\ngrad_input = sigmoid.backward(grad_output)\nprint(f\"Gradient input: {grad_input}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π (sigmoid –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç)\nassert np.all(grad_input >= 0), \"–ì—Ä–∞–¥–∏–µ–Ω—Ç Sigmoid –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\"\n\nprint(\"‚úÖ Sigmoid —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.251349Z","iopub.execute_input":"2025-09-30T12:41:32.251659Z","iopub.status.idle":"2025-09-30T12:41:32.267933Z","shell.execute_reply.started":"2025-09-30T12:41:32.251631Z","shell.execute_reply":"2025-09-30T12:41:32.266972Z"}},"outputs":[{"name":"stdout","text":"Input: [[-10.  -1.   0.   1.  10.]]\nOutput: [[4.5397872e-05 2.6894143e-01 5.0000000e-01 7.3105860e-01 9.9995458e-01]]\nGradient input: [[0.19661194 0.19661194 0.19661194 0.19661194 0.19661194]]\n‚úÖ Sigmoid —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"### –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Tanh\n\nclass Tanh(Layer):\n    def __init__(self):\n        super().__init__()\n        self.output = None\n\n    def forward(self, x):\n        self.output = np.clip((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)), -0.9999, 0.9999)\n        return self.output\n\n    def backward(self, grad_output):\n        return grad_output * (1 - np.square(self.output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.270078Z","iopub.execute_input":"2025-09-30T12:41:32.270381Z","iopub.status.idle":"2025-09-30T12:41:32.294161Z","shell.execute_reply.started":"2025-09-30T12:41:32.270357Z","shell.execute_reply":"2025-09-30T12:41:32.293196Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# –¢–µ—Å—Ç Tanh (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Tanh)\nprint(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Tanh –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\ntanh = Tanh()\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\nx_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n\n# # Forward pass\noutput = tanh.forward(x_test)\nprint(f\"Input: {x_test}\")\nprint(f\"Output: {output}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (-1, 1)\nassert np.all(output > -1) and np.all(output < 1), \"Tanh –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ (-1, 1)\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º –∞–Ω—Ç–∏—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç—å: tanh(-x) = -tanh(x)\nx_antisym = np.array([[2]], dtype=np.float32)\nout_pos = tanh.forward(x_antisym)\nout_neg = tanh.forward(-x_antisym)\nassert np.allclose(out_neg, -out_pos, atol=1e-6), \"Tanh –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∞–Ω—Ç–∏—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ tanh(0) = 0\nzero_out = tanh.forward(np.array([[0]], dtype=np.float32))\nassert np.allclose(zero_out, 0, atol=1e-6), \"tanh(0) –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–µ–Ω 0\"\n\n# # Backward pass\ngrad_output = np.ones_like(output)\ngrad_input = tanh.backward(grad_output)\nprint(f\"Gradient input: {grad_input}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π (tanh –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–µ—Ç)\nassert np.all(grad_input >= 0), \"–ì—Ä–∞–¥–∏–µ–Ω—Ç Tanh –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\"\n\nprint(\"‚úÖ Tanh —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.295329Z","iopub.execute_input":"2025-09-30T12:41:32.295669Z","iopub.status.idle":"2025-09-30T12:41:32.321876Z","shell.execute_reply.started":"2025-09-30T12:41:32.295635Z","shell.execute_reply":"2025-09-30T12:41:32.320530Z"}},"outputs":[{"name":"stdout","text":"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Tanh –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\nInput: [[-10.  -1.   0.   1.  10.]]\nOutput: [[-0.9999    -0.7615942  0.         0.7615942  0.9999   ]]\nGradient input: [[1. 1. 1. 1. 1.]]\n‚úÖ Tanh —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"### –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π\n\nclass Linear(Layer):\n    def __init__(self, input_size, output_size, bias=True):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.use_bias = bias\n\n        ### Kaiming He –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n        self.weight = np.random.normal(loc=0, scale=2/input_size, size=(input_size, output_size)).astype(np.float32)\n\n        if self.use_bias:\n            self.bias = np.zeros(output_size, dtype=np.float32)\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        self.input = x\n\n        output = self.weight.T @ x.T\n        if self.use_bias:\n            output = output.T + self.bias\n        return output\n\n    def backward(self, grad_output):\n        grad_input = grad_output @ self.weight.T\n\n        self.grad_weight = self.input.T @ grad_output\n\n        if self.use_bias:\n            self.grad_bias = np.sum(grad_output, axis=0)\n\n        return grad_input\n\n    def update_weights(self, learning_rate=0.01):\n        if self.grad_weight is not None:\n            self.weight.T -= learning_rate * self.grad_weight\n\n        if self.use_bias and self.grad_bias is not None:\n            self.bias -= learning_rate * self.grad_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.323189Z","iopub.execute_input":"2025-09-30T12:41:32.323526Z","iopub.status.idle":"2025-09-30T12:41:32.352922Z","shell.execute_reply.started":"2025-09-30T12:41:32.323493Z","shell.execute_reply":"2025-09-30T12:41:32.352006Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# –¢–µ—Å—Ç Linear (–∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Linear)\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Linear –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\nlinear = Linear(input_size=3, output_size=2, bias=True)\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É –≤–µ—Å–æ–≤\nassert linear.weight.shape == (3, 2), f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤–µ—Å–æ–≤: {linear.weight.shape}\"\nassert linear.bias.shape == (2,), f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ bias: {linear.bias.shape}\"\n\nprint(f\"–í–µ—Å–∞: \\n{linear.weight}\")\nprint(f\"Bias: {linear.bias}\")\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\nbatch_size = 4\nx_test = np.random.randn(batch_size, 3).astype(np.float32)\n\n# # Forward pass\noutput = linear.forward(x_test)\nexpected_shape = (batch_size, 2)\n\nprint(f\"Input shape: {x_test.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Expected shape: {expected_shape}\")\n\nassert output.shape == expected_shape, f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–∞: {output.shape}\"\n\n# # Backward pass\ngrad_output = np.random.randn(*output.shape).astype(np.float32)\ngrad_input = linear.backward(grad_output)\n\nprint(f\"Gradient input shape: {grad_input.shape}\")\nprint(f\"Gradient weight shape: {linear.grad_weight.shape}\")\nprint(f\"Gradient bias shape: {linear.grad_bias.shape}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\nassert grad_input.shape == x_test.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ –≤—Ö–æ–¥—É\"\nassert linear.grad_weight.shape == linear.weight.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ –≤–µ—Å–∞–º\"\nassert linear.grad_bias.shape == linear.bias.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ bias\"\n\nprint(\"‚úÖ Linear —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.353931Z","iopub.execute_input":"2025-09-30T12:41:32.354270Z","iopub.status.idle":"2025-09-30T12:41:32.408284Z","shell.execute_reply.started":"2025-09-30T12:41:32.354236Z","shell.execute_reply":"2025-09-30T12:41:32.407379Z"}},"outputs":[{"name":"stdout","text":"–í–µ—Å–∞: \n[[-0.08106621 -1.4308075 ]\n [-0.1198379  -1.1728611 ]\n [ 0.21903415  1.1588073 ]]\nBias: [0. 0.]\nInput shape: (4, 3)\nOutput shape: (4, 2)\nExpected shape: (4, 2)\nGradient input shape: (4, 3)\nGradient weight shape: (3, 2)\nGradient bias shape: (2,)\n‚úÖ Linear —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"### –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (Sequential)\n\nclass Sequential(Layer):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = list(layers)\n        self.layer_outputs = [] ### –ù–µ –ø–æ–Ω—è–ª –∑–∞—á–µ–º —ç—Ç–æ –∑–¥–µ—Å—å –Ω—É–∂–Ω–æ, –≤–µ–¥—å –¥–∞–ª—å—à–µ –Ω–∏–≥–¥–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def forward(self, x):\n        self.layer_outputs = []\n        output = x\n        for layer in self.layers:\n            self.layer_outputs.append(output)\n            output = layer.forward(output)\n\n        return output\n\n    def backward(self, grad_output):\n        grad = grad_output\n        for layer in reversed(self.layers):\n            grad = layer.backward(grad)\n        return grad\n\n    def train(self):\n        super().train()\n        for layer in self.layers:\n            layer.train()\n\n    def eval(self):\n        super().eval()\n        for layer in self.layers:\n            layer.eval()\n\n    def __len__(self):\n        return len(self.layers)\n\n    def __getitem__(self, idx):\n        return self.layers[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.409354Z","iopub.execute_input":"2025-09-30T12:41:32.409679Z","iopub.status.idle":"2025-09-30T12:41:32.433955Z","shell.execute_reply.started":"2025-09-30T12:41:32.409650Z","shell.execute_reply":"2025-09-30T12:41:32.432358Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"### Dropout\n\nclass Dropout(Layer):\n    def __init__(self, dropout_rate=0.5):\n        super().__init__()\n        self.dropout_rate = dropout_rate\n        self.mask = None\n\n    def forward(self, x):\n        if self.training:\n            self.mask = np.random.uniform(0, 1, size=np.shape(x))\n            x[self.mask > self.dropout_rate] = 0\n            output = x\n        else:\n            output = x\n            self.mask = None\n        return output\n\n    def backward(self, grad_output):\n        if self.training:\n            grad_output[self.mask > self.dropout_rate] = 0\n            grad_input = grad_output\n        else:\n            grad_input = grad_output\n        return grad_input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.435213Z","iopub.execute_input":"2025-09-30T12:41:32.435728Z","iopub.status.idle":"2025-09-30T12:41:32.462570Z","shell.execute_reply.started":"2025-09-30T12:41:32.435685Z","shell.execute_reply":"2025-09-30T12:41:32.461410Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# –¢–µ—Å—Ç Dropout (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Dropout)\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Dropout –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\ndropout = Dropout(dropout_rate=0.5)\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\nx_test = np.ones((100, 10), dtype=np.float32)\n\n# # –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è\ndropout.train()\noutput_train = dropout.forward(x_test)\n\nprint(f\"–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è:\")\nprint(f\"Input mean: {x_test.mean():.3f}\")\nprint(f\"Output mean: {output_train.mean():.3f}\")\nprint(f\"Proportion of zeros: {(output_train == 0).mean():.3f}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–æ–≤ \"–≤—ã–∫–ª—é—á–µ–Ω–∞\"\nzeros_ratio = (output_train == 0).mean()\nexpected_zeros = 0.5  # dropout_rate\nassert abs(zeros_ratio - expected_zeros) < 0.1, f\"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –¥–æ–ª—è –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {zeros_ratio}\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\nassert abs(output_train.mean() - x_test.mean()) < 0.1, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è\"\n\n# # –¢–µ—Å—Ç –≤ —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\ndropout.eval()\noutput_eval = dropout.forward(x_test)\n\nprint(f\"\\n–†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:\")\nprint(f\"Output mean: {output_eval.mean():.3f}\")\nprint(f\"Proportion of zeros: {(output_eval == 0).mean():.3f}\")\n\n# # –í —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è\nassert np.allclose(output_eval, x_test), \"–í —Ä–µ–∂–∏–º–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –≤—ã—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤—Ö–æ–¥–æ–º\"\n\n# # –¢–µ—Å—Ç backward pass\ndropout.train()\noutput_train = dropout.forward(x_test)\ngrad_output = np.ones_like(output_train)\ngrad_input = dropout.backward(grad_output)\n\nprint(f\"\\nGradient test:\")\nprint(f\"Grad input shape: {grad_input.shape}\")\nprint(f\"Grad input mean: {grad_input.mean():.3f}\")\n\nassert grad_input.shape == x_test.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\"\n\nprint(\"‚úÖ Dropout —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.463874Z","iopub.execute_input":"2025-09-30T12:41:32.464840Z","iopub.status.idle":"2025-09-30T12:41:32.488038Z","shell.execute_reply.started":"2025-09-30T12:41:32.464804Z","shell.execute_reply":"2025-09-30T12:41:32.486972Z"}},"outputs":[{"name":"stdout","text":"–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è:\nInput mean: 0.521\nOutput mean: 0.521\nProportion of zeros: 0.479\n\n–†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:\nOutput mean: 0.521\nProportion of zeros: 0.479\n\nGradient test:\nGrad input shape: (100, 10)\nGrad input mean: 0.501\n‚úÖ Dropout —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"### Batch Normalization\n\nclass BatchNorm(Layer):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        self.gamma = np.ones(num_features, dtype=np.float32)\n        self.beta = np.zeros(num_features, dtype=np.float32)\n\n        self.running_mean = np.zeros(num_features, dtype=np.float32)\n        self.running_var = np.ones(num_features, dtype=np.float32)\n\n        self.batch_mean = None\n        self.batch_var = None\n        self.normalized = None\n        self.input = None\n        self.grad_gamma = None\n        self.grad_beta = None\n\n    def forward(self, x):\n        self.input = x.astype(np.float32)\n        #self.input = x\n\n        if self.training:\n            self.batch_mean = np.mean(x, axis=0, keepdims=True).astype(np.float32)\n            self.batch_var = np.var(x, axis=0, keepdims=True).astype(np.float32)\n\n            self.running_mean = (self.momentum * self.batch_mean + (1 - self.momentum) * self.running_mean).astype(np.float32)\n            self.running_var = (self.momentum * self.batch_var + (1 - self.momentum) * self.running_var).astype(np.float32)\n\n            mean = self.batch_mean\n            var = self.batch_var\n        else:\n            mean = self.running_mean.reshape(1, -1)\n            var = self.running_var.reshape(1, -1)\n\n        self.normalized = ((x - mean) / np.sqrt(var + self.eps)).astype(np.float32)\n\n        output = (self.gamma * self.normalized + self.beta).astype(np.float32)\n\n        return output\n\n    def backward(self, grad_output):\n        batch_size = grad_output.shape[0]\n        grad_output = grad_output.astype(np.float32)\n\n        self.grad_gamma = np.sum(grad_output * self.normalized, axis=0).astype(np.float32)\n        self.grad_beta = np.sum(grad_output, axis=0).astype(np.float32)\n        ### –ò —Ç—É—Ç –Ω–∞—á–∞–ª—Å—è —Å—É—â–∏–π –∫–æ—à–º–∞—Ä\n        ### –ß–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞—é—Å—å, —Å–ø–∏—Å–∞–ª, –Ω–æ –¥–∞–∂–µ —Ç–∞–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è BatchNorm —ç—Ç–æ –∫–∞–∫–∞—è-—Ç–æ –∂–µ—Å—Ç—å\n        ### –°–ø–∏—Å–∞–ª –æ—Ç—Å—é–¥–∞: https://blog.tnichols.org/posts/batchnorm-backward/\n\n        if self.training:\n            grad_normalized = grad_output * self.gamma\n            grad_var = np.sum(grad_normalized * (self.input - self.batch_mean) * -0.5 * \n                            np.power(self.batch_var + self.eps, -1.5), axis=0, keepdims=True)\n            grad_mean = np.sum(grad_normalized * -1.0 / np.sqrt(self.batch_var + self.eps), axis=0, keepdims=True) + \\\n                       grad_var * np.mean(-2.0 * (self.input - self.batch_mean), axis=0, keepdims=True)\n            \n            grad_input = (grad_normalized / np.sqrt(self.batch_var + self.eps) + \n                         grad_var * 2.0 * (self.input - self.batch_mean) / batch_size + \n                         grad_mean / batch_size).astype(np.float32)\n        else:\n            grad_input = (grad_output * self.gamma / np.sqrt(self.running_var.reshape(1, -1) + self.eps)).astype(np.float32)\n        \n        return grad_input\n\n    def update_weights(self, learning_rate=0.001):\n        \"\"\"\n        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ gamma –∏ beta\n        \"\"\"\n        if self.grad_gamma is not None:\n            self.gamma -= learning_rate * self.grad_gamma\n        \n        if self.grad_beta is not None:\n            self.beta -= learning_rate * self.grad_beta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.489258Z","iopub.execute_input":"2025-09-30T12:41:32.489590Z","iopub.status.idle":"2025-09-30T12:41:32.512481Z","shell.execute_reply.started":"2025-09-30T12:41:32.489545Z","shell.execute_reply":"2025-09-30T12:41:32.511221Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# –¢–µ—Å—Ç BatchNorm (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ BatchNorm)\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ BatchNorm –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\nbatch_norm = BatchNorm(num_features=4)\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é\nassert np.allclose(batch_norm.gamma, 1.0), \"Gamma –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –µ–¥–∏–Ω–∏—Ü–∞–º–∏\"\nassert np.allclose(batch_norm.beta, 0.0), \"Beta –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω—É–ª—è–º–∏\"\nassert np.allclose(batch_norm.running_mean, 0.0), \"Running mean –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω—É–ª—è–º–∏\"\nassert np.allclose(batch_norm.running_var, 1.0), \"Running var –¥–æ–ª–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –µ–¥–∏–Ω–∏—Ü–∞–º–∏\"\n\nprint(f\"Gamma: {batch_norm.gamma}\")\nprint(f\"Beta: {batch_norm.beta}\")\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\nx_test = np.array([\n     [1, 2, 3, 4],\n     [2, 3, 4, 5],\n     [3, 4, 5, 6]\n ], dtype=np.float32)\n\nprint(f\"Input: \\n{x_test}\")\nprint(f\"Input mean per feature: {x_test.mean(axis=0)}\")\nprint(f\"Input std per feature: {x_test.std(axis=0)}\")\n\n# # Forward pass –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è\nbatch_norm.train()\noutput = batch_norm.forward(x_test)\n\nprint(f\"\\nOutput: \\n{output}\")\nprint(f\"Output mean per feature: {output.mean(axis=0)}\")\nprint(f\"Output std per feature: {output.std(axis=0)}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—ã—Ö–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω (—Å—Ä–µ–¥–Ω–µ–µ ‚âà 0, std ‚âà 1)\nassert np.allclose(output.mean(axis=0), 0, atol=1e-6), \"–°—Ä–µ–¥–Ω–µ–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ 0\"\nassert np.allclose(output.std(axis=0), 1, atol=1e-6), \"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–ª–∏–∑–∫–æ –∫ 1\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ running —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\nprint(f\"\\nRunning mean: {batch_norm.running_mean}\")\nprint(f\"Running var: {batch_norm.running_var}\")\n\n# # Backward pass\ngrad_output = np.ones_like(output)\ngrad_input = batch_norm.backward(grad_output)\n\nprint(f\"\\nGradient input shape: {grad_input.shape}\")\nprint(f\"Gradient gamma shape: {batch_norm.grad_gamma.shape}\")\nprint(f\"Gradient beta shape: {batch_norm.grad_beta.shape}\")\n\nassert grad_input.shape == x_test.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ –≤—Ö–æ–¥—É\"\nassert batch_norm.grad_gamma.shape == batch_norm.gamma.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ gamma\"\nassert batch_norm.grad_beta.shape == batch_norm.beta.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ beta\"\n\n# # –¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\nbatch_norm.eval()\noutput_eval = batch_norm.forward(x_test)\nprint(f\"\\nInference mode output mean: {output_eval.mean(axis=0)}\")\n\nprint(\"‚úÖ BatchNorm —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.516544Z","iopub.execute_input":"2025-09-30T12:41:32.516939Z","iopub.status.idle":"2025-09-30T12:41:32.547383Z","shell.execute_reply.started":"2025-09-30T12:41:32.516903Z","shell.execute_reply":"2025-09-30T12:41:32.546419Z"}},"outputs":[{"name":"stdout","text":"Gamma: [1. 1. 1. 1.]\nBeta: [0. 0. 0. 0.]\nInput: \n[[1. 2. 3. 4.]\n [2. 3. 4. 5.]\n [3. 4. 5. 6.]]\nInput mean per feature: [2. 3. 4. 5.]\nInput std per feature: [0.8164966 0.8164966 0.8164966 0.8164966]\n\nOutput: \n[[-1.2247356 -1.2247356 -1.2247356 -1.2247356]\n [ 0.         0.         0.         0.       ]\n [ 1.2247356  1.2247356  1.2247356  1.2247356]]\nOutput mean per feature: [0. 0. 0. 0.]\nOutput std per feature: [0.99999243 0.99999243 0.99999243 0.99999243]\n\nRunning mean: [[0.2 0.3 0.4 0.5]]\nRunning var: [[0.96666664 0.96666664 0.96666664 0.96666664]]\n\nGradient input shape: (3, 4)\nGradient gamma shape: (4,)\nGradient beta shape: (4,)\n\nInference mode output mean: [1.8307619 2.746143  3.6615238 4.5769053]\n‚úÖ BatchNorm —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"### Adam –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n\nclass Adam:\n\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.learning_rate = np.float32(learning_rate)\n        self.beta1 = np.float32(beta1)\n        self.beta2 = np.float32(beta2)\n        self.eps = np.float32(eps)\n        \n        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è\n        self.m = {}  # first moment\n        self.v = {}  # second moment\n        self.t = 0   # time step\n\n    def update(self, layer, layer_id):\n        self.t += 1\n\n        if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:\n            if f\"{layer_id}_weight\" not in self.m:\n                self.m[f\"{layer_id}_weight\"] = np.zeros_like(layer.grad_weight, dtype=np.float32)\n                self.v[f\"{layer_id}_weight\"] = np.zeros_like(layer.grad_weight, dtype=np.float32)\n            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–æ–≤\n            self.m[f\"{layer_id}_weight\"] = (self.beta1 * self.m[f\"{layer_id}_weight\"] + (1 - self.beta1) * layer.grad_weight).astype(np.float32)\n            self.v[f\"{layer_id}_weight\"] = (self.beta2 * self.v[f\"{layer_id}_weight\"] + (1 - self.beta2) * np.square(layer.grad_weight)).astype(np.float32)\n            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è\n            m_corrected = self.m[f\"{layer_id}_weight\"] / (1 - self.beta1 ** self.t).astype(np.float32)\n            v_corrected = self.v[f\"{layer_id}_weight\"] / (1 - self.beta2 ** self.t).astype(np.float32)\n            #–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤\n            layer.weight -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps).astype(np.float32)\n\n        if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:\n            if f\"{layer_id}_bias\" not in self.m:\n                self.m[f\"{layer_id}_bias\"] = np.zeros_like(layer.grad_bias, dtype=np.float32)\n                self.v[f\"{layer_id}_bias\"] = np.zeros_like(layer.grad_bias, dtype=np.float32)\n            \n            # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ bias\n            self.m[f\"{layer_id}_bias\"] = (self.beta1 * self.m[f\"{layer_id}_bias\"] + (1 - self.beta1) * layer.grad_bias).astype(np.float32)\n            self.v[f\"{layer_id}_bias\"] = (self.beta2 * self.v[f\"{layer_id}_bias\"] + (1 - self.beta2) * np.square(layer.grad_bias)).astype(np.float32)\n            \n            m_corrected_bias = (self.m[f\"{layer_id}_bias\"] / ( 1 - self.beta1 ** self.t)).astype(np.float32)\n            v_corrected_bias = (self.v[f\"{layer_id}_bias\"] / ( 1 - self.beta2 ** self.t)).astype(np.float32)\n            \n            layer.bias -= (self.learning_rate * m_corrected_bias / (np.sqrt(v_corrected_bias) + self.eps)).astype(np.float32) \n\n    def zero_grad(self, layers):\n        \"\"\"\n        –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n        \"\"\"\n        for layer in layers:\n            if hasattr(layer, 'grad_weight'):\n                layer.grad_weight = None\n            if hasattr(layer, 'grad_bias'):\n                layer.grad_bias = None\n            if hasattr(layer, 'grad_gamma'):\n                layer.grad_gamma = None\n            if hasattr(layer, 'grad_beta'):\n                layer.grad_beta = None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.548397Z","iopub.execute_input":"2025-09-30T12:41:32.548778Z","iopub.status.idle":"2025-09-30T12:41:32.577975Z","shell.execute_reply.started":"2025-09-30T12:41:32.548754Z","shell.execute_reply":"2025-09-30T12:41:32.576665Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# –¢–µ—Å—Ç Adam (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Adam)\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ Adam –∫–ª–∞—Å—Å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\n# # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è\nlayer = Linear(3, 2)\nadam = Adam(learning_rate=0.01)\n\n# # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\nlayer.grad_weight = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype=np.float32)\nlayer.grad_bias = np.array([0.1, 0.2], dtype=np.float32)\n\n# # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤\ninitial_weight = layer.weight.copy()\ninitial_bias = layer.bias.copy()\n\nprint(f\"Initial weight: \\n{initial_weight}\")\nprint(f\"Initial bias: {initial_bias}\")\nprint(f\"Weight gradient: \\n{layer.grad_weight}\")\nprint(f\"Bias gradient: {layer.grad_bias}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é Adam\nassert len(adam.m) == 0, \"–ú–æ–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\"\nassert len(adam.v) == 0, \"–ú–æ–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\"\nassert adam.t == 0, \"Time step –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–≤–µ–Ω 0\"\n\n# # –í—ã–ø–æ–ª–Ω–∏–º –æ–¥–∏–Ω —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\nadam.update(layer, \"test_layer\")\n\nprint(f\"\\nAfter 1 step:\")\nprint(f\"Updated weight: \\n{layer.weight}\")\nprint(f\"Updated bias: {layer.bias}\")\nprint(f\"Time step: {adam.t}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤–µ—Å–∞ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å\nassert not np.allclose(layer.weight, initial_weight), \"–í–µ—Å–∞ –¥–æ–ª–∂–Ω—ã –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\"\nassert not np.allclose(layer.bias, initial_bias), \"Bias –¥–æ–ª–∂–µ–Ω –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –º–æ–º–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã\nassert \"test_layer_weight\" in adam.m, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è –≤–µ—Å–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\nassert \"test_layer_bias\" in adam.m, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è bias –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\nassert \"test_layer_weight\" in adam.v, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è –≤–µ—Å–æ–≤ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\nassert \"test_layer_bias\" in adam.v, \"–ú–æ–º–µ–Ω—Ç –¥–ª—è bias –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—ã –º–æ–º–µ–Ω—Ç–æ–≤\nassert adam.m[\"test_layer_weight\"].shape == layer.weight.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –º–æ–º–µ–Ω—Ç–∞ –≤–µ—Å–æ–≤\"\nassert adam.m[\"test_layer_bias\"].shape == layer.bias.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –º–æ–º–µ–Ω—Ç–∞ bias\"\n\n# # –¢–µ—Å—Ç zero_grad\nadam.zero_grad([layer])\nassert layer.grad_weight is None, \"–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–Ω—É–ª–µ–Ω—ã\"\nassert layer.grad_bias is None, \"–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã bias –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–Ω—É–ª–µ–Ω—ã\"\n\nprint(\"‚úÖ Adam —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.579191Z","iopub.execute_input":"2025-09-30T12:41:32.579492Z","iopub.status.idle":"2025-09-30T12:41:32.608317Z","shell.execute_reply.started":"2025-09-30T12:41:32.579463Z","shell.execute_reply":"2025-09-30T12:41:32.607358Z"}},"outputs":[{"name":"stdout","text":"Initial weight: \n[[0.32108837 0.34717318]\n [0.4521888  1.1152909 ]\n [0.02874159 0.9214675 ]]\nInitial bias: [0. 0.]\nWeight gradient: \n[[0.1 0.2]\n [0.3 0.4]\n [0.5 0.6]]\nBias gradient: [0.1 0.2]\n\nAfter 1 step:\nUpdated weight: \n[[0.31108838 0.3371732 ]\n [0.4421888  1.1052909 ]\n [0.01874159 0.9114675 ]]\nUpdated bias: [-0.01 -0.01]\nTime step: 1\n‚úÖ Adam —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"### –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n\nclass CrossEntropyLoss:\n    def __init__(self):\n        self.predictions = None\n        self.targets = None\n    \n    def forward(self, predictions, targets):\n\n        self.predictions = predictions\n        self.targets = targets\n        \n        # TODO: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ softmax –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n        self.softmax_pred = softmax(predictions)\n        \n        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ cross-entropy loss\n        clipped_softmax = np.clip(self.softmax_pred, 1e-4, 1 + 1e-4)\n        loss = - np.mean(np.log(clipped_softmax[np.arange(predictions.shape[0]), targets]))\n        \n        return loss\n    \n    def backward(self):\n        \"\"\"\n        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ Cross-Entropy Loss\n        \n        Returns:\n            –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n        \"\"\"\n        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç\n        one_hot_targets = one_hot_encode(self.targets, self.predictions.shape[1])\n        \n        grad = (self.softmax_pred - one_hot_targets) / self.predictions.shape[0]\n        \n        return grad\n\n\nclass MSELoss:\n    def __init__(self):\n        self.predictions = None\n        self.targets = None\n    \n    def forward(self, predictions, targets):\n        \"\"\"\n        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Mean Squared Error\n        \n        Args:\n            predictions: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏\n            targets: –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n        \n        Returns:\n            –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n        \"\"\"\n        self.predictions = predictions\n        self.targets = targets\n        \n        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ MSE\n        loss = np.mean(np.square(self.predictions - self.targets))\n        \n        return loss\n    \n    def backward(self):\n        \"\"\"\n        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ MSE\n        \n        Returns:\n            –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º\n        \"\"\"\n        # TODO: –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç MSE\n        grad = 2 * (self.predictions - self.targets) / self.predictions.shape[0]\n        \n        return grad\n\n\ndef softmax(x):\n    \"\"\"\n    –£—Å—Ç–æ–π—á–∏–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è softmax\n    \"\"\"\n    # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ softmax —Ñ—É–Ω–∫—Ü–∏—é\n    x_shifted = x - np.max(x, axis=-1, keepdims=True)\n    exp_x = np.exp(x_shifted)\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\ndef one_hot_encode(labels, num_classes):\n    \"\"\"\n    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –≤ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É\n    \"\"\"\n    # TODO: –°–æ–∑–¥–∞–π—Ç–µ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫—É\n    one_hot = np.zeros((len(labels), num_classes))\n    one_hot[np.arange(len(labels)), labels] = 1\n    return one_hot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.609668Z","iopub.execute_input":"2025-09-30T12:41:32.610675Z","iopub.status.idle":"2025-09-30T12:41:32.640149Z","shell.execute_reply.started":"2025-09-30T12:41:32.610641Z","shell.execute_reply":"2025-09-30T12:41:32.638931Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# –¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å (–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Loss —Ñ—É–Ω–∫—Ü–∏–π)\n#print(\"‚ö†Ô∏è –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤—ã—à–µ, –∑–∞—Ç–µ–º —Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")\n\n# # –¢–µ—Å—Ç CrossEntropyLoss\nprint(\"üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CrossEntropyLoss...\")\nce_loss = CrossEntropyLoss()\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\npredictions = np.array([[2.0, 1.0, 0.1], [1.0, 3.0, 0.2]], dtype=np.float32)\ntargets = np.array([0, 1], dtype=np.int32)\n\nprint(f\"Predictions: \\n{predictions}\")\nprint(f\"Targets: {targets}\")\n\n# # Forward pass\nloss_value = ce_loss.forward(predictions, targets)\nprint(f\"CrossEntropy Loss: {loss_value:.4f}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ loss –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π\nassert loss_value > 0, \"CrossEntropy loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º\"\n\n# # Backward pass\ngrad = ce_loss.backward()\nprint(f\"Gradient shape: {grad.shape}\")\nprint(f\"Gradient: \\n{grad}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\nassert grad.shape == predictions.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ CrossEntropy\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —Å—É–º–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º —Ä–∞–≤–Ω–∞ 0 (—Å–≤–æ–π—Å—Ç–≤–æ softmax)\nassert np.allclose(grad.sum(axis=1), 0, atol=1e-6), \"–°—É–º–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 0\"\n\nprint(\"‚úÖ CrossEntropyLoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n\n# # –¢–µ—Å—Ç MSELoss\nprint(\"\\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MSELoss...\")\nmse_loss = MSELoss()\n\n# # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\npredictions_reg = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\ntargets_reg = np.array([[1.5, 2.5], [2.5, 3.5]], dtype=np.float32)\n\nprint(f\"Predictions: \\n{predictions_reg}\")\nprint(f\"Targets: \\n{targets_reg}\")\n\n# # Forward pass\nmse_value = mse_loss.forward(predictions_reg, targets_reg)\nprint(f\"MSE Loss: {mse_value:.4f}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ loss –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π\nassert mse_value >= 0, \"MSE loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\"\n\n# # Backward pass\ngrad_mse = mse_loss.backward()\nprint(f\"MSE Gradient shape: {grad_mse.shape}\")\nprint(f\"MSE Gradient: \\n{grad_mse}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\nassert grad_mse.shape == predictions_reg.shape, \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ MSE\"\n\nprint(\"‚úÖ MSELoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n\n# # –¢–µ—Å—Ç softmax —Ñ—É–Ω–∫—Ü–∏–∏\nprint(\"\\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Softmax...\")\nx_softmax = np.array([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]], dtype=np.float32)\nsoftmax_output = softmax(x_softmax)\n\nprint(f\"Input: \\n{x_softmax}\")\nprint(f\"Softmax output: \\n{softmax_output}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ —Å—É–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Ä–∞–≤–Ω–∞ 1\nassert np.allclose(softmax_output.sum(axis=1), 1.0), \"–°—É–º–º–∞ softmax –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–≤–Ω–∞ 1\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ\nassert np.all(softmax_output > 0), \"–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è softmax –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏\"\nassert np.all(softmax_output < 1), \"–í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è softmax –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–µ–Ω—å—à–µ 1\"\n\nprint(\"‚úÖ Softmax —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n\n# # –¢–µ—Å—Ç one-hot encoding\nprint(\"\\nüè∑Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ One-hot encoding...\")\nlabels = np.array([0, 2, 1, 0])\none_hot = one_hot_encode(labels, num_classes=3)\n\nprint(f\"Labels: {labels}\")\nprint(f\"One-hot: \\n{one_hot}\")\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º —Ñ–æ—Ä–º—É\nassert one_hot.shape == (4, 3), \"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ one-hot –∫–æ–¥–∏—Ä–æ–≤–∫–∏\"\n\n# # –ü—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–æ–≤–Ω–æ –æ–¥–Ω—É –µ–¥–∏–Ω–∏—Ü—É\nassert np.all(one_hot.sum(axis=1) == 1), \"–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–Ω—É –µ–¥–∏–Ω–∏—Ü—É\"\n\nprint(\"‚úÖ One-hot encoding —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\")\n\nprint(\"\\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.641186Z","iopub.execute_input":"2025-09-30T12:41:32.641527Z","iopub.status.idle":"2025-09-30T12:41:32.683452Z","shell.execute_reply.started":"2025-09-30T12:41:32.641495Z","shell.execute_reply":"2025-09-30T12:41:32.682419Z"}},"outputs":[{"name":"stdout","text":"üî• –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CrossEntropyLoss...\nPredictions: \n[[2.  1.  0.1]\n [1.  3.  0.2]]\nTargets: [0 1]\nCrossEntropy Loss: 0.2981\nGradient shape: (2, 3)\nGradient: \n[[-0.17049941  0.1212165   0.04928295]\n [ 0.05657142 -0.08199063  0.02541918]]\n‚úÖ CrossEntropyLoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n\nüìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ MSELoss...\nPredictions: \n[[1. 2.]\n [3. 4.]]\nTargets: \n[[1.5 2.5]\n [2.5 3.5]]\nMSE Loss: 0.2500\nMSE Gradient shape: (2, 2)\nMSE Gradient: \n[[-0.5 -0.5]\n [ 0.5  0.5]]\n‚úÖ MSELoss —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Softmax...\nInput: \n[[1. 2. 3.]\n [1. 1. 1.]]\nSoftmax output: \n[[0.09003057 0.24472848 0.66524094]\n [0.33333334 0.33333334 0.33333334]]\n‚úÖ Softmax —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n\nüè∑Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ One-hot encoding...\nLabels: [0 2 1 0]\nOne-hot: \n[[1. 0. 0.]\n [0. 0. 1.]\n [0. 1. 0.]\n [1. 0. 0.]]\n‚úÖ One-hot encoding —Ç–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω!\n\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"class NeuralNetwork:\n    def __init__(self):\n        # TODO: –°–æ–∑–¥–∞–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏\n        self.model = Sequential(Linear(784, 512),\n                                BatchNorm(512),\n                                ReLU(),\n                                Dropout(dropout_rate=0.5),\n                                Linear(512, 256),\n                                BatchNorm(256),\n                                ReLU(),\n                                Dropout(dropout_rate=0.3),\n                                Linear(256, 128),\n                                BatchNorm(128),\n                                ReLU(),\n                                Dropout(dropout_rate=0.2),\n                                Linear(128, 10)\n\n        )\n    \n    def forward(self, x):\n        return self.model.forward(x)\n    \n    def backward(self, grad_output):\n        return self.model.backward(grad_output)\n    \n    def train(self):\n        self.model.train()\n    \n    def eval(self):\n        self.model.eval()\n    \n    def get_trainable_layers(self):\n        \"\"\"\n        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n        \"\"\"\n        trainable_layers = []\n        for layer in self.model.layers:\n            if hasattr(layer, 'update_weights'):\n                trainable_layers.append(layer)\n        return trainable_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.684992Z","iopub.execute_input":"2025-09-30T12:41:32.685310Z","iopub.status.idle":"2025-09-30T12:41:32.713199Z","shell.execute_reply.started":"2025-09-30T12:41:32.685288Z","shell.execute_reply":"2025-09-30T12:41:32.711978Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:32.714517Z","iopub.execute_input":"2025-09-30T12:41:32.714919Z","iopub.status.idle":"2025-09-30T12:41:38.563331Z","shell.execute_reply.started":"2025-09-30T12:41:32.714890Z","shell.execute_reply":"2025-09-30T12:41:38.562184Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"device = torch.device('cuda') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:38.564441Z","iopub.execute_input":"2025-09-30T12:41:38.565018Z","iopub.status.idle":"2025-09-30T12:41:38.570434Z","shell.execute_reply.started":"2025-09-30T12:41:38.564976Z","shell.execute_reply":"2025-09-30T12:41:38.569447Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model = NeuralNetwork()  ### –ù–µ –ø–æ–Ω—è–ª –∫–∞–∫ –Ω–∞ device –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏\ncriterion = CrossEntropyLoss()\noptimizer = Adam(learning_rate=0.001)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndataset = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\nX = dataset.drop('label', axis=1).values.astype(np.float32) / 255.0\ny = dataset['label'].values\n\n# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)\n\nprint(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}\")\nprint(f\"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_val.shape}\")\n\nX_train = X_train.astype(np.float32)\nX_val = X_val.astype(np.float32)\ny_train = y_train.astype(np.int64)\ny_val = y_val.astype(np.int64)\n\n#/kaggle/input/digit-recognizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:50.138257Z","iopub.execute_input":"2025-09-30T12:41:50.138592Z","iopub.status.idle":"2025-09-30T12:41:53.314689Z","shell.execute_reply.started":"2025-09-30T12:41:50.138567Z","shell.execute_reply":"2025-09-30T12:41:53.313095Z"}},"outputs":[{"name":"stdout","text":"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: (33600, 784)\n–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: (8400, 784)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def create_batches(X, y, batch_size, shuffle=True):\n    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    for start_idx in range(0, n_samples, batch_size):\n        end_idx = min(start_idx + batch_size, n_samples)\n        batch_indices = indices[start_idx:end_idx]\n        yield X[batch_indices], y[batch_indices]\n\ndef calculate_accuracy(predictions, targets):\n    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏\"\"\"\n    predicted_classes = np.argmax(predictions, axis=1)\n    return np.mean(predicted_classes == targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:54.579017Z","iopub.execute_input":"2025-09-30T12:41:54.579449Z","iopub.status.idle":"2025-09-30T12:41:54.588937Z","shell.execute_reply.started":"2025-09-30T12:41:54.579421Z","shell.execute_reply":"2025-09-30T12:41:54.587746Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val, \n                num_epochs=30, batch_size=128, print_every=100, max_grad_norm=1.0):\n    \"\"\"\n    –¶–∏–∫–ª —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏\n    \"\"\"\n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n    \n    for epoch in range(num_epochs):\n        # –†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n        model.train()\n        epoch_train_loss = 0.0\n        epoch_train_acc = 0.0\n        num_batches = 0\n\n        indices = np.random.permutation(len(X_train))\n        X_train_shuffled = X_train[indices]\n        y_train_shuffled = y_train[indices]\n        \n        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∞—Ö\n        for batch_X, batch_y in create_batches(X_train_shuffled, y_train_shuffled, batch_size):\n            # Forward pass\n            predictions = model.forward(batch_X)\n            loss = criterion.forward(predictions, batch_y)\n            \n            # Backward pass\n            grad_output = criterion.backward()\n            model.backward(grad_output)\n            \n            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n            total_norm = 0\n            trainable_layers = model.get_trainable_layers()\n            for layer in trainable_layers:\n                if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:\n                    total_norm += np.sum(layer.grad_weight ** 2)\n                if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:\n                    total_norm += np.sum(layer.grad_bias ** 2)\n            total_norm = np.sqrt(total_norm)\n\n            if total_norm > max_grad_norm:\n                clip_coef = max_grad_norm / (total_norm + 1e-6)\n                for layer in trainable_layers:\n                    if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:\n                        layer.grad_weight *= clip_coef\n                    if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:\n                        layer.grad_bias *= clip_coef\n\n            for i, layer in enumerate(trainable_layers):\n                optimizer.update(layer, f\"layer_{i}\")\n            \n            # –û–±–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n            optimizer.zero_grad(model.model.layers)\n            \n            \n            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n            epoch_train_loss += loss\n            epoch_train_acc += calculate_accuracy(predictions, batch_y)\n            num_batches += 1\n            \n            if num_batches % print_every == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{num_batches}], \"\n                      f\"Loss: {loss:.4f}, Accuracy: {calculate_accuracy(predictions, batch_y):.4f}\")\n        \n        # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ —ç–ø–æ—Ö—É\n        epoch_train_loss /= num_batches\n        epoch_train_acc /= num_batches\n        \n        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n        model.eval()\n        val_predictions = model.forward(X_val)\n        val_loss = criterion.forward(val_predictions, y_val)\n        val_acc = calculate_accuracy(val_predictions, y_val)\n        \n        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n        train_losses.append(epoch_train_loss)\n        train_accuracies.append(epoch_train_acc)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    return train_losses, train_accuracies, val_losses, val_accuracies\n\n# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\nprint(\"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\ntrain_losses, train_accs, val_losses, val_accs = train_model(\n    model, optimizer, criterion, X_train, y_train, X_val, y_val,\n    num_epochs=40, batch_size=256, print_every=50\n)\n\nprint(\"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntest_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\nX_test = test_data.values.astype(np.float32) / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ [0, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:43.921310Z","iopub.status.idle":"2025-09-30T12:41:43.921737Z","shell.execute_reply.started":"2025-09-30T12:41:43.921521Z","shell.execute_reply":"2025-09-30T12:41:43.921542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_on_test_data(model, X_test, batch_size=256):\n\n    model.eval()\n    \n    all_predictions = []\n    num_samples = X_test.shape[0]\n\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_X = X_test[start_idx:end_idx]\n\n        batch_predictions = model.forward(batch_X)\n        \n        batch_probs = softmax(batch_predictions)\n        \n        batch_classes = np.argmax(batch_probs, axis=1)\n        \n        all_predictions.extend(batch_classes)\n    \n    return np.array(all_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:43.922672Z","iopub.status.idle":"2025-09-30T12:41:43.922941Z","shell.execute_reply.started":"2025-09-30T12:41:43.922819Z","shell.execute_reply":"2025-09-30T12:41:43.922831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_submission_file(predictions, filename='submission.csv'):\n    submission = pd.DataFrame({\n        'ImageId': range(1, len(predictions) + 1),  # ImageId –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 1\n        'Label': predictions\n    })\n\n    submission.to_csv(filename, index=False)\n    return submission\n\nsubmission_df = create_submission_file(predictions, 'mnist_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:41:43.923440Z","iopub.status.idle":"2025-09-30T12:41:43.923688Z","shell.execute_reply.started":"2025-09-30T12:41:43.923573Z","shell.execute_reply":"2025-09-30T12:41:43.923584Z"}},"outputs":[],"execution_count":null}]}